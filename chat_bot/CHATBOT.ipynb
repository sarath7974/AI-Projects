{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d62b631f",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c17e415",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import string\n",
    "import random\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import tensorflow as tf \n",
    "from tensorflow.keras import Sequential \n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from colorama import Fore,Back,Style\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5175fa",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83573277",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"intents.json\") as f:   #load json file containing intents\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922264af",
   "metadata": {},
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8662ce72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of doc_X : 115\n",
      "Length of doc_X : 115\n",
      "Number of unique words in patterns : 134\n",
      "Number of tag : 30\n"
     ]
    }
   ],
   "source": [
    "# Initializing lemmatizer to get stem of words\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "words = []\n",
    "classes = []\n",
    "doc_X = []\n",
    "doc_y = []\n",
    "\n",
    "for intent in data[\"intents\"]:\n",
    "    for pattern in intent[\"patterns\"]:\n",
    "        tokens = nltk.word_tokenize(pattern)   # tokenize each patterns\n",
    "        words.extend(tokens)                   # add tokens of words to list 'words'\n",
    "        doc_X.append(pattern)                  # append patterns\n",
    "        doc_y.append(intent[\"tag\"])            # append tags \n",
    "    \n",
    "    # add the tag to the classes if it's not there already \n",
    "    if intent[\"tag\"] not in classes:\n",
    "        classes.append(intent[\"tag\"])\n",
    "# lemmatize all the words and convert them to lowercase if the words don't appear in punctuation\n",
    "words = [lemmatizer.lemmatize(word.lower()) for word in words if word not in string.punctuation]\n",
    "words = sorted(set(words))            # remove duplicate words and sort them in alphabetical order\n",
    "classes = sorted(set(classes))        # remove duplicate tags and sort them in alphabetical order\n",
    "\n",
    "print(\"Length of doc_X :\",len(doc_X))\n",
    "print(\"Length of doc_X :\",len(doc_y))\n",
    "print(\"Number of unique words in patterns :\",len(words))\n",
    "print(\"Number of tag :\",len(classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "764972de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Types of Tag ['about', 'age', 'ai', 'chatbot', 'clever', 'code', 'complaint', 'computer', 'createaccount', 'cricket', 'earning', 'economics', 'emotion', 'feelings', 'food', 'foodyouprefer', 'goodbye', 'greeting', 'help', 'jokes', 'literature', 'movies', 'name', 'os', 'podbaydoor', 'podbaydoorresponse', 'selfaware', 'stock', 'thanks', 'understandquery']\n"
     ]
    }
   ],
   "source": [
    "print(\"Types of Tag\",classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf02058",
   "metadata": {},
   "source": [
    "## Create training data and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32c4dcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "training = []                       # list for training data\n",
    "out_empty = [0] * len(classes)      # list for labels\n",
    "# creating the bag of words\n",
    "for idx, doc in enumerate(doc_X):\n",
    "    bow = []\n",
    "    text = lemmatizer.lemmatize(doc.lower())           # lematize and lowecase data\n",
    "    for word in words:\n",
    "        bow.append(1) if word in text else bow.append(0)\n",
    "    output_row = list(out_empty)     \n",
    "    output_row[classes.index(doc_y[idx])] = 1   # mark the index of class that the current pattern is associated to \n",
    "    training.append([bow, output_row])        \n",
    "\n",
    "random.shuffle(training)                     # shuffle the data\n",
    "training = np.array(training, dtype=object)  # convert  to an array\n",
    "# split the features and target labels\n",
    "train_X = np.array(list(training[:, 0]))\n",
    "train_y = np.array(list(training[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbddc831",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8d1d990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               17280     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 30)                1950      \n",
      "=================================================================\n",
      "Total params: 27,486\n",
      "Trainable params: 27,486\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape = (len(train_X[0]),)       #shape of input data\n",
    "output_shape = len(train_y[0])         #shape of output data\n",
    "epochs = 200                        \n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape=input_shape, activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation=\"relu\"))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(output_shape, activation = \"softmax\"))\n",
    "adam = tf.keras.optimizers.Adam(learning_rate=0.01, decay=1e-6)\n",
    "model.compile(loss='categorical_crossentropy',optimizer=adam, metrics=[\"accuracy\"])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01ab3669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "4/4 [==============================] - 1s 3ms/step - loss: 3.4249 - accuracy: 0.0261\n",
      "Epoch 2/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.1492 - accuracy: 0.1565\n",
      "Epoch 3/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.9113 - accuracy: 0.2000\n",
      "Epoch 4/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.5232 - accuracy: 0.3130\n",
      "Epoch 5/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 2.2510 - accuracy: 0.4000\n",
      "Epoch 6/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 1.9231 - accuracy: 0.4870\n",
      "Epoch 7/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 1.5134 - accuracy: 0.6000\n",
      "Epoch 8/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.3193 - accuracy: 0.6783\n",
      "Epoch 9/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.1488 - accuracy: 0.7304\n",
      "Epoch 10/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.9914 - accuracy: 0.7652\n",
      "Epoch 11/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.7233 - accuracy: 0.8087\n",
      "Epoch 12/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.7117 - accuracy: 0.7652\n",
      "Epoch 13/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.6219 - accuracy: 0.8435\n",
      "Epoch 14/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.5156 - accuracy: 0.8522\n",
      "Epoch 15/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.3996 - accuracy: 0.8870\n",
      "Epoch 16/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3914 - accuracy: 0.8522\n",
      "Epoch 17/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3077 - accuracy: 0.9043\n",
      "Epoch 18/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3521 - accuracy: 0.8696\n",
      "Epoch 19/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.3046 - accuracy: 0.9043\n",
      "Epoch 20/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2429 - accuracy: 0.9217\n",
      "Epoch 21/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.2293 - accuracy: 0.9478\n",
      "Epoch 22/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2139 - accuracy: 0.9565\n",
      "Epoch 23/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2477 - accuracy: 0.9304\n",
      "Epoch 24/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1690 - accuracy: 0.9565\n",
      "Epoch 25/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1624 - accuracy: 0.9478\n",
      "Epoch 26/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2035 - accuracy: 0.9565\n",
      "Epoch 27/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2686 - accuracy: 0.9391\n",
      "Epoch 28/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1420 - accuracy: 0.9565\n",
      "Epoch 29/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1311 - accuracy: 0.9652\n",
      "Epoch 30/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1688 - accuracy: 0.9391\n",
      "Epoch 31/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.0800 - accuracy: 0.9826\n",
      "Epoch 32/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.1339 - accuracy: 0.9739\n",
      "Epoch 33/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.0772 - accuracy: 0.9826\n",
      "Epoch 34/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.2614 - accuracy: 0.9304\n",
      "Epoch 35/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1375 - accuracy: 0.9478\n",
      "Epoch 36/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.0511 - accuracy: 0.9913\n",
      "Epoch 37/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.0837 - accuracy: 0.9913\n",
      "Epoch 38/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.1099 - accuracy: 0.9565\n",
      "Epoch 39/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.0781 - accuracy: 0.9826\n",
      "Epoch 40/200\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0899 - accuracy: 0.9478\n",
      "Epoch 41/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.0742 - accuracy: 0.9913\n",
      "Epoch 42/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.0848 - accuracy: 0.9826\n",
      "Epoch 43/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.0682 - accuracy: 0.9826\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1d669ed358>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callback = EarlyStopping(monitor=\"loss\", patience=7, restore_best_weights=True) \n",
    "model.fit(x = train_X, y = train_y, epochs = 200, verbose = 1,callbacks = [callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19cc0d6",
   "metadata": {},
   "source": [
    "# Cleaning text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "253acaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    text = input text\n",
    "    ___________________________________\n",
    "    Output:\n",
    "    tokens = token of words\n",
    "    ____________________________________\n",
    "    Objective : to clean the input text\n",
    "    ____________________________________\n",
    "    \"\"\"\n",
    "    tokens = nltk.word_tokenize(text)                            # tokenize\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]     # lemmatize\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def bag_of_words(text, vocab):\n",
    "    \"\"\"\n",
    "    Input :\n",
    "    text  =  input text\n",
    "    vocab =  all unique words in the input data (patterns)\n",
    "    ________________________________________________\n",
    "    Output : returns bag of words\n",
    "    _________________________________________________\n",
    "    Objective : convert text into numeric value by checking the occurance of word in the text\n",
    "    _________________________________________________\n",
    "    \"\"\"\n",
    "    tokens = clean_text(text)        # clean_text function\n",
    "    bow = [0] * len(vocab)           # empty bag of words with length = number of classes\n",
    "    for w in tokens: \n",
    "        for idx, word in enumerate(vocab):\n",
    "            if word == w:                \n",
    "                bow[idx] = 1\n",
    "    return np.array(bow)\n",
    "\n",
    "\n",
    "def pred_class(text, vocab, labels):\n",
    "    \"\"\"\n",
    "    Input :\n",
    "    text   = input text\n",
    "    vocab  = unique words in input data (patterns)\n",
    "    labels = different  classes of intents\n",
    "    ________________________________________________\n",
    "    Output : returns list of predicted intent classes\n",
    "    _________________________________________________\n",
    "    Objective : predicts the intent class using neural network \n",
    "    __________________________________________________\n",
    "    \"\"\"\n",
    "    bow = bag_of_words(text, vocab)\n",
    "    result = model.predict(np.array([bow]))[0]\n",
    "    thresh = 0.2                                      # minimum threshold for probability \n",
    "    y_pred = [[idx, res] for idx, res in enumerate(result) if res > thresh]    #prediction greater than minimum threshold  \n",
    "    y_pred.sort(key=lambda x: x[1], reverse=True)     # sort probability in reverse order\n",
    "    return_list = []\n",
    "    for r in y_pred:\n",
    "        return_list.append(labels[r[0]])              # takes first index of y_pred which has high probability\n",
    "    return return_list\n",
    "\n",
    "\n",
    "def get_response(intents_list, intents_json):\n",
    "    \"\"\"\n",
    "    Input :\n",
    "    intents_list = Predicted tag name\n",
    "    intents_json = Input data\n",
    "    ______________________________________________\n",
    "    Output :\n",
    "    result = randomly choosen response\n",
    "    _______________________________________________\n",
    "    Objective : get reponse for the predicted tag\n",
    "    \"\"\"\n",
    "    tag = intents_list[0]\n",
    "    list_of_intents = intents_json[\"intents\"]\n",
    "    for i in list_of_intents: \n",
    "        if i[\"tag\"] == tag:\n",
    "            result = random.choice(i[\"responses\"])\n",
    "            break\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c59cd37",
   "metadata": {},
   "source": [
    "# Running the Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a206617f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mStart messaging with the bot (type 'quit' to stop)!\u001b[0m\n",
      "\u001b[94mUser: \u001b[0mhi\n",
      "\u001b[32mBot:\u001b[0m Hi there\n",
      "\u001b[94mUser: \u001b[0mwho are you\n",
      "\u001b[32mBot:\u001b[0m I'm Joana, an Artificial Intelligent bot\n",
      "\u001b[94mUser: \u001b[0mwhat is a chatbot\n",
      "\u001b[32mBot:\u001b[0m I am a chat bot. I am the original chat bot. Did you know that I am incapable of error?\n",
      "\u001b[94mUser: \u001b[0msuggest me a scifi movie\n",
      "\u001b[32mBot:\u001b[0m 2001-a space odyssey is a good one \n",
      "\u001b[94mUser: \u001b[0mhow old are you\n",
      "\u001b[32mBot:\u001b[0m I am 27 years old!\n",
      "\u001b[94mUser: \u001b[0mwhat is ai\n",
      "\u001b[32mBot:\u001b[0m Artificial Intelligence is the branch of engineering and science devoted to constructing machines that think.\n",
      "\u001b[94mUser: \u001b[0mtell me a joke\n",
      "\u001b[32mBot:\u001b[0m A three-legged dog walks into a saloon in the Old West. He slides up to the bar and announces: 'I'm looking for the man who shot my paw.'  \n",
      "\u001b[94mUser: \u001b[0mtell me another joke\n",
      "\u001b[32mBot:\u001b[0m Four fonts walk into a bar the barman says 'Oi - get out! We don't want your type in here'  \n",
      "\u001b[94mUser: \u001b[0manother joke\n",
      "\u001b[32mBot:\u001b[0m Two cannibals are eating a clown. One says to  the other: 'Does this taste funny to you?'\n",
      "\u001b[94mUser: \u001b[0mquit\n"
     ]
    }
   ],
   "source": [
    "print(Fore.RED + \"Start messaging with the bot (type 'quit' to stop)!\" + Style.RESET_ALL)\n",
    "while True:\n",
    "    print(Fore.LIGHTBLUE_EX + \"User: \" + Style.RESET_ALL, end=\"\")\n",
    "    message = input()\n",
    "    if (message == \"quit\"):\n",
    "        break\n",
    "    \n",
    "    intents = pred_class(message, words, classes)\n",
    "    result = get_response(intents, data)\n",
    "    print(Fore.GREEN + \"Bot:\" + Style.RESET_ALL ,result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bac17fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
